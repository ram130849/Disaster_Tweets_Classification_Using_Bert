{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4ZP82sS2WiW"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjxliOtj2Zci"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchtext torchaudio\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HWG5kwe2ZhG"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkYvUfTE2Zk3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,random_split,DataLoader,RandomSampler,SequentialSampler\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from transformers import AdamW \n",
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "from transformers import BertTokenizer as bt\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7GvQyWL2Zoj"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "print('Number of training sentences: ', len(df))\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uE8XIEK-2ZsZ"
   },
   "outputs": [],
   "source": [
    "# Print some negative sample tweets\n",
    "for txt in df[df.target==0].text.sample(10).values:\n",
    "  print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMpSo-_R2Zyc"
   },
   "outputs": [],
   "source": [
    "# Print some negative sample tweets\n",
    "for txt in df[df.target==1].text.sample(10).values:\n",
    "  print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq6whOmd2Z2r"
   },
   "outputs": [],
   "source": [
    "df.text.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8MowbNj2Z7e"
   },
   "outputs": [],
   "source": [
    "print(\"Positive data: {:.2f}%\".format(len(df[df.target==1])*100/len(df)))\n",
    "print(\"Negative data: {:.2f}%\".format(len(df[df.target==0])*100/len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQM84yjQ2Z-5"
   },
   "outputs": [],
   "source": [
    "tweets = df.text.values\n",
    "labels = df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoswJ07c2aCz"
   },
   "outputs": [],
   "source": [
    "print(\"{} out of {} tweets have a http://... link within itself. ({:.2f}%)\".format(len([t for t in tweets if \"http://\" in t]),len(tweets),len([t for t in tweets if \"http://\" in t])*100/len(tweets))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36mn2bsD2aG2"
   },
   "outputs": [],
   "source": [
    "# Print tweets which has url's in it.\n",
    "[t for t in tweets if \"http://\" in t][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhVnFB1S2aLD"
   },
   "outputs": [],
   "source": [
    "# Print tweets which have URL's at the end of the tweets\n",
    "[t for t in [t for t in tweets if \"http://\" in t] if \"http://\" in t.split()[-1]][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VjATxdf2aO-"
   },
   "outputs": [],
   "source": [
    "# Print no of tweets which has a @ in tweets\n",
    "print(len([t for t in tweets if \"@\" in t]))\n",
    "[t for t in tweets if \"@\" in t][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjxec0fB2aTJ"
   },
   "outputs": [],
   "source": [
    "print(\"{} out of {} tweets have a @ user_id tag within itself. ({:.2f}%)\".format(len([t for t in tweets if \"@\" in t]),len(tweets),len([t for t in tweets if \"@\" in t])*100/len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fyw0HRF2aXC"
   },
   "outputs": [],
   "source": [
    "print(\"percentage of POSITIVE samples containing @user_id tag: {:.2f}%\".format(len([t for t in df[df['target']==1]['text'] if \"@\" in t])*100/len(df[df['target']==1])))\n",
    "print(\"percentage of NEGATIVE samples containing @user_id tag: {:.2f}%\".format(len([t for t in df[df['target']==0]['text'] if \"@\" in t])*100/len(df[df['target']==0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tmvm-_9Z2abl"
   },
   "outputs": [],
   "source": [
    "print(\"{} out of {} tweets has # tag within itself ({:.2f}%)\".format(len([t for t in tweets if \"#\" in t]),len(tweets),len([t for t in tweets if \"#\" in t])*100/len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NsZFfbs2afh"
   },
   "outputs": [],
   "source": [
    "print(\"percentage of POSITIVE samples containing # hash_tag: {:.2f}%\".format(len([t for t in df[df['target']==1]['text'] if \"#\" in t])*100/len(df[df['target']==1])))\n",
    "print(\"percentage of NEGATIVE samples containing # hash_tag: {:.2f}%\".format(len([t for t in df[df['target']==0]['text'] if \"#\" in t])*100/len(df[df['target']==0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbldbiRA2ajY"
   },
   "outputs": [],
   "source": [
    "tokenizer = bt.from_pretrained(\"bert-base-uncased\",do_lower_case=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfmcRjhRAuWE"
   },
   "outputs": [],
   "source": [
    "print(' Tweets and Labels: ', tweets[0], labels[0])\n",
    "print('Bert Tokenizer output: ', tokenizer.tokenize(tweets[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5clbOHcl2anV"
   },
   "outputs": [],
   "source": [
    "tweets = [\" \".join([word if 'http://' not in word else \"http\" for word in t.split()]) for t in tweets]\n",
    "tweets[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VOcof362arK"
   },
   "outputs": [],
   "source": [
    "print(' Original: ', tweets[-17])\n",
    "print('   Target: ', labels[-17])\n",
    "print('Tokenized: ', tokenizer.tokenize(tweets[-17]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MteS7wzL2auk"
   },
   "outputs": [],
   "source": [
    "tweets = [\" \".join([word if '@' not in word else word.replace(\"@\", \" \") for word in t.split()]) for t in tweets]\n",
    "tweets[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTFORksz2ayA"
   },
   "outputs": [],
   "source": [
    "enc_tweets = [tokenizer.encode(t) for t in tweets]\n",
    "lens = np.array([len(t) for t in tweets])\n",
    "\n",
    "print(\"# of Sentences:\",len(tweets))\n",
    "print(\"Max Sentence Length:\",max(lens))\n",
    "print(\"Average Sentence Length:\",np.mean(lens))\n",
    "print(\"Median Sentence Length:\",np.median(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyW2b9OS2a12"
   },
   "outputs": [],
   "source": [
    "unique = list(set(lens))\n",
    "unique.sort()\n",
    "count = [sum([1 if l==u else 0 for l in lens]) for u in unique]\n",
    "plt.bar(unique, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x64xQvxk2a5x"
   },
   "outputs": [],
   "source": [
    "def encode(sentences,labels,tokenize,max_len):\n",
    "      encode_dicts = []\n",
    "      for sentence in sentences:\n",
    "          encode_dicts.append(tokenize.encode_plus(sentence,        # sentence to encode.\n",
    "                                                   add_special_tokens=True, # append <cls>,<sep> token. \n",
    "                                                   max_length = max_len, # append maximum length to the sentence.\n",
    "                                                   padding='max_length', # append <pad> token till maximum length.\n",
    "                                                   return_attention_mask=True, # construct attention mask\n",
    "                                                   return_tensors='pt')) # tensor to return.\n",
    "      input_ids = torch.cat([d['input_ids'] for d in encode_dicts],dim=0)\n",
    "      attention_mask = torch.cat([d['attention_mask'] for d in encode_dicts],dim=0)\n",
    "      labels = torch.tensor(labels)\n",
    "\n",
    "      return input_ids,attention_mask,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll6sx-zO2a-A"
   },
   "outputs": [],
   "source": [
    "input_ids,attention_masks,labels = encode(tweets,labels,tokenizer,max_len=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUBUFpRD2bBR"
   },
   "outputs": [],
   "source": [
    "print('Original: ', tweets[1],'\\n')\n",
    "print('Token IDs:', input_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bn8yOeAe2bEu"
   },
   "outputs": [],
   "source": [
    "print(len(input_ids[0]))\n",
    "tokenizer.convert_ids_to_tokens(input_ids[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4iDfoSWFl43"
   },
   "outputs": [],
   "source": [
    "def makeDataLoader(input_ids,attention_masks,labels,split=True):\n",
    "      dataset = TensorDataset(input_ids,attention_masks,labels)\n",
    "      if(split):\n",
    "        train_size = int(0.6*len(dataset))\n",
    "        val_size = len(dataset)-train_size\n",
    "        trainData,valData = random_split(dataset,[train_size,val_size])\n",
    "      else:\n",
    "        trainData = dataset\n",
    "      batch_size = 32\n",
    "\n",
    "      if(split):\n",
    "        train_dataloader = DataLoader(trainData,sampler = RandomSampler(trainData),batch_size = batch_size)\n",
    "        valid_dataloader = DataLoader(valData,sampler = SequentialSampler(valData),batch_size = batch_size)\n",
    "        return train_dataloader,valid_dataloader\n",
    "      else:\n",
    "        train_dataloader = DataLoader(trainData,sampler = RandomSampler(trainData),batch_size = batch_size)\n",
    "      return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNKo8nUBFl93"
   },
   "outputs": [],
   "source": [
    "train_dataloader, validation_dataloader = makeDataLoader(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGCTS1QjFmGP"
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2,output_attentions=False,output_hidden_states=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZLI_bC2FmKV"
   },
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6cZTxUQFmOz"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (not pytorch)- 'W'= 'Weight Decay fix\"\n",
    "optimizer = AdamW(\n",
    "                    model.parameters(),\n",
    "                    lr = 5e-5,         # default \n",
    "                    eps = 1e-8,\n",
    "                    no_deprecation_warning=True         # default \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3ApARmQFmS9"
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(preds,labels):\n",
    "      flat_pred = np.argmax(preds,axis=1).flatten()\n",
    "      flat_labels = labels.flatten()\n",
    "      return np.sum(flat_pred==flat_labels)/len(flat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpWnfBtnFmXK"
   },
   "outputs": [],
   "source": [
    "def rand_seed():\n",
    "    seed_val = 50\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2YlOJofFmbU"
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_time = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xccyuvb9Fmfw"
   },
   "outputs": [],
   "source": [
    "len(train_dataloader), len(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlutRbBkFmko"
   },
   "outputs": [],
   "source": [
    "def train_bert(train_loader,val_loader,model,optimizer,n_epochs,output_hidden=0):\n",
    "      rand_seed()\n",
    "      total_time  = time.time()\n",
    "      training_stats = []\n",
    "      hidden_states = []\n",
    "\n",
    "      n_tr_btchs = len(train_loader)\n",
    "      scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=n_tr_btchs*n_epochs)\n",
    "\n",
    "      for i in range(n_epochs):\n",
    "          start_time = time.time()\n",
    "          total_train_loss,total_train_accuracy = 0,0\n",
    "          model.train()\n",
    "\n",
    "          for step,batch in enumerate(train_loader):\n",
    "              input_ids,attn_masks,labels = batch\n",
    "              model.zero_grad()\n",
    "              if(output_hidden):\n",
    "                  print(\"Inside If - Training\")\n",
    "                  print(model(input_ids,\n",
    "                                        token_type_ids=None,\n",
    "                                        attention_mask = attn_masks,\n",
    "                                        labels = labels))\n",
    "                  loss, logits, h = model(input_ids,\n",
    "                                        token_type_ids=None,\n",
    "                                        attention_mask = attn_masks,\n",
    "                                        labels = labels)\n",
    "                  h = [layer.detach().cpu().numpy() for layer in h]\n",
    "                  if(i==n_epochs-1):\n",
    "                      hidden_states.append(h[-1])\n",
    "              else:  \n",
    "                  print(\"Inside Else - Training\")\n",
    "                  loss = model(input_ids,\n",
    "                                      token_type_ids=None,\n",
    "                                      attention_mask= attn_masks,\n",
    "                                      labels=labels)['loss']\n",
    "                  logits = model(input_ids,\n",
    "                                      token_type_ids=None,\n",
    "                                      attention_mask= attn_masks,\n",
    "                                      labels=labels)['logits']\n",
    "              total_train_loss+=loss.item()\n",
    "              total_train_accuracy+=flat_accuracy(logits.detach().cpu().numpy(),labels.detach().cpu().numpy())\n",
    "              loss.backward()\n",
    "              torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "              optimizer.step()\n",
    "              scheduler.step()\n",
    "          print(\"Epoch: {}/{}\".format((i+1), n_epochs),\n",
    "              \"  Train loss: {0:.4f}\".format(total_train_loss/n_tr_btchs),\n",
    "              \"  Train Acc: {0:.4f}\".format(total_train_accuracy/n_tr_btchs),\n",
    "              \"  ({:})\".format(format_time(time.time() - start_time)))\n",
    "          training_stats.append(\"Epoch: {}/{}\".format((i+1), n_epochs),\n",
    "              \"  Train loss: {0:.4f}\".format(total_train_loss/n_tr_btchs),\n",
    "              \"  Train Acc: {0:.4f}\".format(total_train_accuracy/n_tr_btchs),\n",
    "              \"  ({:})\".format(format_time(time.time() - start_time)))\n",
    "          if val_loader is not None:\n",
    "              n_valid_btchs = len(val_loader)\n",
    "              start_time = time.time()\n",
    "              model.eval()\n",
    "              total_eval_accuracy,total_eval_loss = 0,0\n",
    "\n",
    "              for step,batch in enumerate(val_loader):\n",
    "                  input_ids,attn_masks,labels = batch\n",
    "                  with torch.no_grad():\n",
    "                    if(output_hidden):\n",
    "                        print(\"Inside If - Validation\")\n",
    "                        loss,logits,val_h = model(input_ids,token_type_ids=None,attention_mask=attn_masks,labels=labels)\n",
    "                        val_h = [layer.detach().cpu().numpy() for layer in val_h]\n",
    "                    else:\n",
    "                        print(\"Inside Else - Validation\")\n",
    "                        loss = model(input_ids,token_type_ids=None,attention_mask = attn_masks,labels = labels)['loss']\n",
    "                        logits = model(input_ids,token_type_ids=None,attention_mask = attn_masks,labels = labels)['logits']\n",
    "                  total_eval_loss+=loss.item()\n",
    "                  logits = logits.detach().cpu().numpy()\n",
    "                  labels = labels.detach().cpu().numpy()\n",
    "                  total_eval_accuracy+=flat_accuracy(logits,labels)\n",
    "              print(\"Valid Loss: {0:.4f}\".format(total_eval_loss/n_valid_btchs),\n",
    "                  \"Valid Acc: {0:.4f}\".format(total_eval_accuracy/n_valid_btchs),\n",
    "                  \"({:})\".format(format_time(time.time()-start_time)))\n",
    "\n",
    "              training_stats.append({'Valid. Loss':total_eval_loss/n_valid_btchs,\n",
    "                                    'Valid. Acc':   total_eval_accuracy/n_valid_btchs,\n",
    "                                    'Validation Time': format_time(time.time()-start_time)})\n",
    "      print(\"\\nTraining complete.\")\n",
    "      print(\"Duration: {:} (h:mm:ss)\".format(format_time(time.time()-total_time)))\n",
    "\n",
    "      if output_hidden:\n",
    "        return training_stats, hidden_states\n",
    "      else:\n",
    "        return training_stats        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRT-Xvt1Fmow"
   },
   "outputs": [],
   "source": [
    "training_stats = train_bert(train_dataloader, validation_dataloader, \n",
    "                            model=model, optimizer=optimizer, \n",
    "                            n_epochs=2,output_hidden=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhnO9rCyFmr9"
   },
   "outputs": [],
   "source": [
    "train_dataloader = makeDataLoader(input_ids, attention_masks, labels, split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sGXySMXFmwm"
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "              \"bert-base-uncased\",          # 12-layer BERT base model, w/ uncased vocab\n",
    "              num_labels = 2,               # number of output labels (2 for binary classification)  \n",
    "              output_attentions = False,    # Whether the model returns attentions weights.\n",
    "              output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgNifxMhFm0Q"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(  model.parameters(), lr = 5e-5, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bURLSpDFm4S"
   },
   "outputs": [],
   "source": [
    "training_stats = train_bert(train_dataloader, None,\n",
    "                            model=model, optimizer=optimizer, \n",
    "                            n_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afqM2ixsRTO5"
   },
   "source": [
    "# Preparing Test Data for Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTeZLn_TFm8R"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_sentences = test_df.text.values\n",
    "test_sentences = [\" \".join([word if('http://' or 'https://') not in word else 'http' for word in t.split()]) for t in test_sentences]\n",
    "test_sentences = [\"\".join([word for word in t.split() if '@' not in word]) for t in test_sentences]\n",
    "test_encoded_sentences = [tokenizer.encode(sentence) for sentence in test_sentences]\n",
    "test_sent_lens = np.array([len(s) for s in test_encoded_sentences])\n",
    "\n",
    "print(\"# of sentences:\",len(test_sentences))\n",
    "print('Max Sentence Length',max(test_sent_lens))\n",
    "print('Avg sentence Length',np.mean(test_sent_lens))\n",
    "print('Median sentence Length',np.median(test_sent_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rjmj2x-mFnAS"
   },
   "outputs": [],
   "source": [
    "test_encoded_dicts = [tokenizer.encode_plus( sent,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length = 100,\n",
    "                                             pad_to_max_length = True,\n",
    "                                             return_attention_mask = True,\n",
    "                                             return_tensors='pt') for sent in test_sentences]\n",
    "input_ids = [d['input_ids'] for d in test_encoded_dicts]\n",
    "input_ids = torch.cat(input_ids,dim=0)\n",
    "attention_masks = [d['attention_mask'] for d in test_encoded_dicts]\n",
    "attention_masks = torch.cat(attention_masks,dim=0)\n",
    "prediction_data = TensorDataset(input_ids,attention_masks)\n",
    "prediction_dataloader = DataLoader(dataset = prediction_data,\n",
    "                                   sample = SequentialSampler(prediction_data),\n",
    "                                   batch_size=32)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dOJU7WsFnEU"
   },
   "outputs": [],
   "source": [
    "len(prediction_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7ud4f1vFnH7"
   },
   "outputs": [],
   "source": [
    "print('Predicting Labels for {:} Test Sentences'.format(len(input_ids)))\n",
    "model.eval()\n",
    "predictions,true_labels = [],[]\n",
    "for batch in prediction_dataloader:\n",
    "    batch = tuple(token for token in batch)\n",
    "    b_input_ids,b_input_mask = batch\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask)['logits']\n",
    "    logits = logits[0].detach().cpu().numpy()\n",
    "    predictions.append(logits)\n",
    "\n",
    "print('Predictions Done!....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIlNpjQMFnLY"
   },
   "outputs": [],
   "source": [
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNTyyqPXFnPC"
   },
   "outputs": [],
   "source": [
    "print(flat_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDlhL3xdFnT2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goaK6tGyFnXs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-m51lcekFna5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2NUmzM_FnfW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmIDePqgFnjK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dr7QUmkfFnoR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23QpQDB7FnsU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9pU-EbwFnwg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUpP-5qpFnzj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GTZBwcNFn3f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nG238DiIFn67"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEwo_PiAFn_-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vV_4C4CYFoEA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLn159BFFoIE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_I3yiCRpFoMF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YAGzjIgFoSk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdHlfPtUFoYn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdpBT_hgFoc9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDKLnWRcFohM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIHPq0JsFomA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cd-csYVFoqH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EV0O9V5kFouh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cx0HGD9hFoyI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GruMDz49Fo2y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-WUsUG5Fo6h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxI3S6FvFo_K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uudHxkYlFpDh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQYWHl1uFpHc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7Zk0gAyFpLF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgLE5vu_FpQn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbE9n5MeFpag"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kW7bHAxMFpi8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mggkQ-p0Fpml"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wHHDXpEFpoK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0opkh6KlFpsm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3Ia4D57FpxD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNN544hOFp1b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyP25VZUFp6C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NKP0y4GFp-F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQES24GUFqBq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTMe3BcDFqF8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGSS_BGPFqLY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b_uNgfyFqPq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAkfvsMiFqbf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yq7YAAFmFqfb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "disaster_tweets_classification_using_bert.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
